{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "vt9B5f7NRIda",
    "outputId": "df2d65c5-bd49-49a5-9ef8-c5470408b53e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.14.0-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |################################| 1.1 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.14.0 typeguard-2.13.3\n",
      "\u001b[33mWARNING: You are using pip version 20.3.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/tarique/myvenv/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_EiplKjw4dok"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kXPJ-c7F4fja"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout, BatchNormalization, Concatenate, Activation\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_UiH-5IQRMQa"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dXrnVUXj7tT3"
   },
   "outputs": [],
   "source": [
    "from src.data import DataHelper\n",
    "from src.data.metrics import map_at_k, recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v9Qx0k0ZKRup"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tJxGhYB45S5V"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f2qZdUFqgtPF"
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5mjYRXj54tY1"
   },
   "outputs": [],
   "source": [
    "RANK_EMBEDDING_DIM = 64\n",
    "BATCH_SIZE = 2**12\n",
    "NEPOCH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ImqKrQio47V1"
   },
   "outputs": [],
   "source": [
    "TRAIN_WEEK_NUM = 6\n",
    "WEEK_NUM = TRAIN_WEEK_NUM + 2\n",
    "\n",
    "VERSION_NAME = \"pivot\"\n",
    "TEST = False  # * Set as `False` when do local experiments to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MS0MEj1z5Rxs"
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"../src/data/\")\n",
    "model_dir = Path(\"../src/data/interim/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHelper(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tbasc4Yn7vjP"
   },
   "outputs": [],
   "source": [
    "data = dh.load_data(name=\"encoded_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Dvqsn-x2NSVR"
   },
   "outputs": [],
   "source": [
    "inter = pd.read_parquet(data_dir / \"interim/processed_inter_v3.pqt\")\n",
    "inter = inter.loc[(inter.t_dat <= \"2020-08-05\")]\n",
    "data[\"inter\"] = inter\n",
    "\n",
    "article_cluster = pd.read_parquet(data_dir/'articles_new.parquet')\n",
    "# https://www.kaggle.com/code/beezus666/k-means-and-feature-importance-for-articles/notebook?scriptVersionId=94269787\n",
    "\n",
    "itemid2idx = pickle.load(open(data_dir/\"index_id_map/item_id2index.pkl\", \"rb\"))\n",
    "article_cluster['article_id'] = article_cluster['article_id'].map(itemid2idx)\n",
    "article_cluster = article_cluster.rename(columns={'department_no':'department_no_cluster', 'ct':'cluster'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvLo7j94F0l6"
   },
   "source": [
    "## Calculate & Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Load pre-trained embeddings\n",
    "w2v_user_embd = np.load(data_dir/'external'/'w2v_user_embd.npy', allow_pickle=True)\n",
    "w2v_item_embd = np.load(data_dir/'external'/'w2v_item_embd.npy', allow_pickle=True)\n",
    "# w2v_product_embd = np.load(data_dir/'external'/'w2v_product_embd.npy', allow_pickle=True)\n",
    "# image_item_embd = np.load(data_dir/'external'/'image_embd.npy', allow_pickle=True)\n",
    "# w2v_sg_user_embd = np.load(data_dir/'external'/'w2v_skipgram_user_embd.npy', allow_pickle=True)\n",
    "# w2v_sg_item_embd = np.load(data_dir/'external'/'w2v_skipgram_item_embd.npy', allow_pickle=True)\n",
    "# w2v_sg_product_embd = np.load(data_dir/'external'/'w2v_skipgram_product_embd.npy', allow_pickle=True)\n",
    "\n",
    "dssm_user_embd = np.load(data_dir/'external'/'dssm_user_embd.npy', allow_pickle=True)\n",
    "dssm_item_embd = np.load(data_dir/'external'/'dssm_item_embd.npy', allow_pickle=True)\n",
    "yt_user_embd = np.load(data_dir/'external'/'yt_user_embd.npy', allow_pickle=True)\n",
    "yt_item_embd = np.load(data_dir/'external'/'yt_item_embd.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7KbLtnk4z0z",
    "outputId": "e42e3afb-631e-4edc-df54-f18741e64d40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:16<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "just_pred = False\n",
    "\n",
    "candidates = {}\n",
    "labels = {}\n",
    "\n",
    "if just_pred:\n",
    "    for i in tqdm(range(1, 2)):\n",
    "        if i==0 and not TEST:\n",
    "            continue\n",
    "        candidates[i] = pd.read_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{i}_candidate_full.pqt\")    \n",
    "        if i != 0:\n",
    "            tmp_label = pd.read_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{i}_label.pqt\")\n",
    "            labels[i] = tmp_label\n",
    "        else:\n",
    "            labels[i] = None\n",
    "    \n",
    "else:\n",
    "    for i in tqdm(range(1, WEEK_NUM)):\n",
    "        if i==0 and not TEST:\n",
    "            continue\n",
    "        candidates[i] = pd.read_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{i}_candidate_full.pqt\")\n",
    "\n",
    "        if i != 0:\n",
    "            tmp_label = pd.read_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{i}_label.pqt\")\n",
    "            labels[i] = tmp_label\n",
    "        else:\n",
    "            labels[i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TkNhkfuq5EPw"
   },
   "outputs": [],
   "source": [
    "feats = [\n",
    "    x\n",
    "    for x in candidates[1].columns\n",
    "    if x\n",
    "    not in [\n",
    "        \"label\",\n",
    "        \"sales_channel_id\",\n",
    "        \"t_dat\",\n",
    "        \"week\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "ids = [\"customer_id\", \"article_id\", \"product_code\"]\n",
    "dense_feats = [x for x in feats if x not in ids]\n",
    "# feats = ids + cat_features + dense_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIfny2Isklqs",
    "outputId": "f6c014fb-4317-4864-fdc1-9df8ccb2c86b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:03<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(dense_feats):\n",
    "    for i in range(1, WEEK_NUM):\n",
    "        if f in candidates[i].columns:\n",
    "            candidates[i][f] = candidates[i][f].astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bYWW99nD556M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.concat([candidates[i] for i in range(1,WEEK_NUM)], ignore_index=True)\n",
    "full_data = full_data[feats+['week','label']]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = full_data[full_data['week']>1]\n",
    "valid = full_data[full_data['week']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVcq5-t7zreT",
    "outputId": "fa0fa69d-fbad-4913-ecf1-c14da162492d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del candidates\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pRJNcFdr9-45"
   },
   "outputs": [],
   "source": [
    "# Standardize\n",
    "# for feat in dense_feats:\n",
    "    # mask = train[feat].notnull()\n",
    "    # value = train.loc[mask, feat].mean()\n",
    "    # train[feat] = train[feat].fillna(value)\n",
    "    # valid[feat] = valid[feat].fillna(value)\n",
    "    # scaler = MinMaxScaler().fit(train[feat].values.reshape(-1,1))\n",
    "    # train[feat] = scaler.transform(train[feat].values.reshape(-1,1))\n",
    "    # valid[feat] = scaler.transform(valid[feat].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "t8-Ohirc6SWH"
   },
   "outputs": [],
   "source": [
    "# feat_dim = {}\n",
    "# for feat in ids:\n",
    "#     if feat in data['user'].columns:\n",
    "#         feat_dim[feat] = int(data['user'][feat].max()) + 1\n",
    "#     elif feat in data['item'].columns:\n",
    "#         feat_dim[feat] = int(data['item'][feat].max()) + 1\n",
    "#     elif feat in article_cluster.columns:\n",
    "#         feat_dim[feat] = int(article_cluster[feat].max()) + 1\n",
    "#     else:\n",
    "#         feat_dim[feat] = int(full_data[feat].max()) + 1\n",
    "        \n",
    "feat_dim = {}\n",
    "for feat in ids:\n",
    "    if feat in data['user'].columns:\n",
    "        feat_dim[feat] = int(data['user'][feat].max())\n",
    "    elif feat in data['item'].columns:\n",
    "        feat_dim[feat] = int(data['item'][feat].max())\n",
    "    elif feat in article_cluster.columns:\n",
    "        feat_dim[feat] = int(article_cluster[feat].max())\n",
    "    else:\n",
    "        feat_dim[feat] = int(full_data[feat].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9cISY640Y8x",
    "outputId": "a4675d31-3ec3-4a9e-ee32-d7891f7c5703"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del full_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqVs0eQC0FRY",
    "outputId": "c69b6014-82da-4613-cacb-4b6e3af3845e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:28,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "X_train1 = train[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
    "X_train2 = np.zeros((X_train1.shape[0], len(dense_feats)), dtype='float32')\n",
    "\n",
    "for i,f in tqdm(enumerate(dense_feats)):\n",
    "    X_train2[:, i] = np.nan_to_num(train[f].values).astype('float32')\n",
    "    del train[f]\n",
    "y_train = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54XxZ1OP0Scb",
    "outputId": "95cd385a-8c70-4914-e05e-3b0cb181b14c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:31,  3.17it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test1 = valid[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
    "X_test2 = np.zeros((X_test1.shape[0], len(dense_feats)), dtype='float32')\n",
    "\n",
    "for i,f in tqdm(enumerate(dense_feats)):\n",
    "    X_test2[:, i] = np.nan_to_num(valid[f].values).astype('float32')\n",
    "    del valid[f]\n",
    "y_test = valid['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxUVZlxJ0Y8y"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "B1ZH721w5pSG"
   },
   "outputs": [],
   "source": [
    "customer_embd_layer_1 = Embedding(\n",
    "    feat_dim[\"customer_id\"], 128, weights=[dssm_user_embd], trainable=False\n",
    ")\n",
    "\n",
    "customer_embd_layer_2 = Embedding(\n",
    "    feat_dim[\"customer_id\"], 128, weights=[w2v_user_embd], trainable=False\n",
    ")\n",
    "\n",
    "customer_embd_layer_3 = Embedding(\n",
    "    feat_dim[\"customer_id\"], 128, weights=[yt_user_embd], trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "1S2eMLNp5H5x"
   },
   "outputs": [],
   "source": [
    "article_embd_layer_1 = Embedding(\n",
    "    feat_dim[\"article_id\"], 128, weights=[dssm_item_embd], trainable=False\n",
    ")\n",
    "\n",
    "article_embd_layer_2 = Embedding(\n",
    "    feat_dim[\"article_id\"], 128, weights=[w2v_item_embd], trainable=False\n",
    ")\n",
    "\n",
    "article_embd_layer_3 = Embedding(\n",
    "    feat_dim[\"article_id\"], 128, weights=[yt_item_embd], trainable=False\n",
    ")\n",
    "\n",
    "# article_embd_layer_4 = Embedding(\n",
    "#     feat_dim[\"article_id\"], 128, weights=[tfidf_item2], trainable=False\n",
    "# )\n",
    "\n",
    "# article_embd_layer_5 = Embedding(\n",
    "#     feat_dim[\"article_id\"], 512, weights=[image_item_embd], trainable=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "bfru5_xrg8qh"
   },
   "outputs": [],
   "source": [
    "# product_embd_layer_1 = Embedding(\n",
    "#     feat_dim[\"product_code\"], 64, weights=[w2v_sg_product_embd], trainable=False\n",
    "# )\n",
    "# product_embd_layer_2 = Embedding(\n",
    "#     feat_dim[\"product_code\"], 64, weights=[w2v_product_embd], trainable=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "n_oDxhjt7rR6"
   },
   "outputs": [],
   "source": [
    "class FM(tf.keras.layers.Layer):\n",
    "    \"\"\"Factorization Machine\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.linear = None\n",
    "        self.w_0 = None\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.linear = Dense(1, use_bias=False)\n",
    "        self.w_0 = self.add_weight(\n",
    "            shape=(1,),\n",
    "            initializer=tf.keras.initializers.Zeros,\n",
    "            dtype=tf.float32,\n",
    "            trainable=True,\n",
    "            name=\"W_0\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, mask=None, *args, **kwargs):\n",
    "        # * inputs: (batch_size, num_of_fields, embedding_dim)\n",
    "        # * part2: (batch_size, 1)\n",
    "        part2 = tf.reduce_sum(self.linear(inputs), axis=1, keepdims=False)\n",
    "\n",
    "        # * square_sum: (batch_size, embedding_dim)\n",
    "        # * sum_square: (batch_size, embedding_dim)\n",
    "        square_sum = tf.square(tf.reduce_sum(inputs, axis=1, keepdims=False))\n",
    "        sum_square = tf.reduce_sum(inputs * inputs, axis=1, keepdims=False)\n",
    "        \n",
    "        # * part3: (batch_size, 1)\n",
    "        part3 = square_sum - sum_square\n",
    "        part3 = 0.5 * tf.reduce_sum(part3, axis=1, keepdims=True)\n",
    "        return tf.nn.bias_add(part2 + part3, self.w_0)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18967385, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psg5LLL94URR",
    "outputId": "98d3578a-f818-48fd-ae50-85d557542880",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer weight shape (1371980, 128) not compatible with provided weight shape (1371981, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-71ecb555c6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_c_id1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustomer_embd_layer_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_c_id2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustomer_embd_layer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx_c_id3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustomer_embd_layer_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myvenv/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 977\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myvenv/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1115\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myvenv/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myvenv/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    884\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myvenv/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2668\u001b[0m         \u001b[0;31m# Using `init_scope` since we want variable assignment in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# `set_weights` to be treated like variable initialization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2670\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2671\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myvenv/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1799\u001b[0m           raise ValueError(\n\u001b[1;32m   1800\u001b[0m               \u001b[0;34m'Layer weight shape %s not compatible with provided weight '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1801\u001b[0;31m               'shape %s' % (ref_shape, weight_shape))\n\u001b[0m\u001b[1;32m   1802\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m         \u001b[0mweight_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer weight shape (1371980, 128) not compatible with provided weight shape (1371981, 128)"
     ]
    }
   ],
   "source": [
    "inputs1 = Input(shape=X_train1.shape[1:], dtype=tf.int64)\n",
    "inputs2 = Input(shape=X_train2.shape[1:], dtype=tf.float32)\n",
    "input1 = tf.cast(inputs1, dtype=tf.int64)\n",
    "\n",
    "x_c_id1 = customer_embd_layer_1(input1[:,0])\n",
    "x_c_id2 = customer_embd_layer_2(input1[:,0])\n",
    "x_c_id3 = customer_embd_layer_3(input1[:,0])\n",
    "\n",
    "x_a_id1 = article_embd_layer_1(input1[:,1])\n",
    "x_a_id2 = article_embd_layer_2(input1[:,1])\n",
    "x_a_id3 = article_embd_layer_3(input1[:,1])\n",
    "x_a_id3 = Dense(128)(x_a_id3)\n",
    "# x_a_id4 = article_embd_layer_4(input1[:,1])\n",
    "# x_a_id5 = article_embd_layer_5(input1[:,1])\n",
    "# x_a_id5 = Dense(128)(x_a_id5)\n",
    "\n",
    "# x_p_id1 = product_embd_layer_1(input1[:,2])\n",
    "# x_p_id2 = product_embd_layer_2(input1[:,2])\n",
    "\n",
    "\n",
    "x_id = Concatenate(axis=-1)([\n",
    "    x_c_id1, x_c_id2,\n",
    "    x_a_id1, x_a_id2, x_a_id3, \n",
    "#     x_a_id4, x_a_id5,\n",
    "#     x_p_id1, x_p_id2,\n",
    "])\n",
    "\n",
    "x0 = Concatenate(axis=-1)([x_id, BatchNormalization()(inputs2)])\n",
    "# x = Dropout(0.2)(x0)\n",
    "# x = Dense(1024, activation='swish')(x)\n",
    "x = Dropout(0.2)(x0)\n",
    "x = Dense(512, activation='swish')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation='swish')(x)\n",
    "\n",
    "x = Concatenate(axis=-1)([x, x0])\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# x_c_id2_expand = tf.expand_dims(x_c_id2, axis=1)\n",
    "# x_a_id2_expand = tf.expand_dims(x_a_id2, axis=1)\n",
    "# x_p_id2_expand = tf.expand_dims(x_p_id2, axis=1)\n",
    "# fm_output = FM()(Concatenate(axis=1)([x_c_id2_expand, x_a_id2_expand, x_p_id2_expand]))\n",
    "# output = output + fm_output\n",
    "# output = Activation('sigmoid')(output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputs1, inputs2], outputs=[output])\n",
    "model.summary()\n",
    "    \n",
    "model.compile(\n",
    "    tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4),\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Um4beR62FC"
   },
   "outputs": [],
   "source": [
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=10, mode='max')\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=model_dir/'model_nn.h5',\n",
    "#     save_weights_only=True,\n",
    "#     monitor='val_auc',\n",
    "#     mode='max',\n",
    "#     save_best_only=True)\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train1, X_train2], y_train.astype(int), \n",
    "#     shuffle=True,\n",
    "#     batch_size=2048,\n",
    "#     validation_data=([X_test1, X_test2], y_test.astype(int)),\n",
    "#     epochs=30,\n",
    "#     callbacks=[checkpoint, early_stop]\n",
    "# )\n",
    "# # 0.7114\n",
    "# # 0.7294\n",
    "# # 0.7382\n",
    "# # 0.7565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGW114IL7I9C"
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_dir/'model_nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXOcPpTD9S0z"
   },
   "outputs": [],
   "source": [
    "probs = model.predict([X_test1, X_test2], batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3qRJElkI2Jk"
   },
   "outputs": [],
   "source": [
    "label = data['inter'][data['inter']['t_dat']>='2020-09-16']\n",
    "label = label.groupby('customer_id')['article_id'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POdGXQWpJicG"
   },
   "outputs": [],
   "source": [
    "valid['prob'] = probs\n",
    "pred = valid.sort_values(by='prob',ascending=False).reset_index(drop=True)\n",
    "pred = pred.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
    "pred.columns = ['customer_id','prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMl0xTnfc2D_"
   },
   "outputs": [],
   "source": [
    "valid = valid[['customer_id','article_id','prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFYhQSVPc8nJ"
   },
   "outputs": [],
   "source": [
    "valid.to_parquet(data_dir/'external'/'nn_valid.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWxE9Yp7JvRa"
   },
   "outputs": [],
   "source": [
    "label = label.merge(pred, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfQX9y5iJy6x",
    "outputId": "eab718fc-3385-4d8d-c2a1-f0697e62cb6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03129488004637625"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_at_k(label['article_id'], label['prediction'], k=12)\n",
    "# 0.028500554033301987\n",
    "# 0.029904528760153\n",
    "\n",
    "# 0.031648009478868075\n",
    "# 0.031309369857160076\n",
    "\n",
    "# 031769005497044554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bk7Z1JIsSkFW"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRkjESDvSlrr"
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_dir/'model_nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMSekWFSRO11"
   },
   "outputs": [],
   "source": [
    "class TQDMPredictCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, custom_tqdm_instance=None, tqdm_cls=tqdm, **tqdm_params):\n",
    "        super().__init__()\n",
    "        self.tqdm_cls = tqdm_cls\n",
    "        self.tqdm_progress = None\n",
    "        self.prev_predict_batch = None\n",
    "        self.custom_tqdm_instance = custom_tqdm_instance\n",
    "        self.tqdm_params = tqdm_params\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        self.tqdm_progress.update(batch - self.prev_predict_batch)\n",
    "        self.prev_predict_batch = batch\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        self.prev_predict_batch = 0\n",
    "        if self.custom_tqdm_instance:\n",
    "            self.tqdm_progress = self.custom_tqdm_instance\n",
    "            return\n",
    "\n",
    "        total = self.params.get('steps')\n",
    "        if total:\n",
    "            total -= 1\n",
    "\n",
    "        self.tqdm_progress = self.tqdm_cls(total=total, **self.tqdm_params)\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        if self.tqdm_progress and not self.custom_tqdm_instance:\n",
    "            self.tqdm_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpTOgjxSSqRJ",
    "outputId": "03b8c3be-b153-4be8-8c3f-45a24c281f7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, valid, X_train1, X_train2, X_test1, X_test2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYGqgcXb0ZWa"
   },
   "outputs": [],
   "source": [
    "chunk = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Dd8Mk_UdfcV",
    "outputId": "4b95775a-d083-4e4b-e68f-743f45f9ec5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [01:12<00:00,  1.33it/s]\n",
      "96it [01:26,  1.12it/s]\n",
      "100%|██████████| 13503/13503 [03:07<00:00, 71.85it/s]\n"
     ]
    }
   ],
   "source": [
    "test_candidates = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week0_candidate_{chunk}.pqt\")\n",
    "for f in tqdm(dense_feats):\n",
    "    test_candidates[f] = test_candidates[f].astype('float16')\n",
    "test1 = test_candidates[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
    "test2 = np.zeros((test1.shape[0], len(dense_feats)), dtype='float32')\n",
    "for i,f in tqdm(enumerate(dense_feats)):\n",
    "    test2[:, i] = np.nan_to_num(test_candidates[f].values).astype('float32')\n",
    "    del test_candidates[f]\n",
    "gc.collect()\n",
    "\n",
    "probs = model.predict([test1, test2], batch_size=2048, callbacks=[TQDMPredictCallback()])\n",
    "# test_candidates = pd.concat([test_candidates, test_candidates2], ignore_index=True)\n",
    "test_candidates[\"prob\"] = probs\n",
    "pred_lgb = test_candidates[['customer_id','article_id','prob']]\n",
    "# pred_lgb = pred_lgb.sort_values(by=[\"customer_id\",\"prob\"], ascending=False).reset_index(drop=True)\n",
    "pred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\n",
    "# pred_lgb = pred_lgb.drop_duplicates(['customer_id', 'prediction'], keep='first')\n",
    "pred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofTfBe310hD3"
   },
   "outputs": [],
   "source": [
    "pred_lgb.to_parquet(data_dir/'interim'/f'nn_test_{chunk}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83ftWTZN5z95"
   },
   "outputs": [],
   "source": [
    "test_pred1 = pd.read_parquet(data_dir/'interim'/f'nn_test_0.pqt')\n",
    "test_pred2 = pd.read_parquet(data_dir/'interim'/f'nn_test_1.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgEvjLJvYFry"
   },
   "outputs": [],
   "source": [
    "test_pred = pd.concat([test_pred1, test_pred2], ignore_index=True)\n",
    "test_pred = test_pred.sort_values(by=[\"prob\"], ascending=False).reset_index(drop=True)\n",
    "test_pred = test_pred.drop_duplicates(['customer_id', 'prediction'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjx-3arOaAx6"
   },
   "outputs": [],
   "source": [
    "test_pred.to_parquet(data_dir/'processed'/'nn_test.pqt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
