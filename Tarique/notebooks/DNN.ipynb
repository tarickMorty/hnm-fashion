{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpEWg9Oq0bbO"
   },
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/Wp-Zhang/HandyRec.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vt9B5f7NRIda",
    "outputId": "df2d65c5-bd49-49a5-9ef8-c5470408b53e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEw9Y3BMiZkY",
    "outputId": "bf1ada5f-04d5-4323-c2e5-53cec1295283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EiplKjw4dok"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/drive/MyDrive/HM-new/\")\n",
    "# sys.path.append(\"./HandyRec/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXPJ-c7F4fja"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout, BatchNormalization, Concatenate, Activation\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UiH-5IQRMQa"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmu45C0u0Y8q"
   },
   "outputs": [],
   "source": [
    "# from handyrec.layers.core import DNN\n",
    "# from handyrec.layers.utils import concat\n",
    "# from handyrec.features import DenseFeature, SparseFeature, FeatureGroup, FeaturePool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXrnVUXj7tT3"
   },
   "outputs": [],
   "source": [
    "from src.data import DataHelper\n",
    "from src.data.metrics import map_at_k, recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9Qx0k0ZKRup"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJxGhYB45S5V"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2qZdUFqgtPF"
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qm-qODKs_dbC"
   },
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mjYRXj54tY1"
   },
   "outputs": [],
   "source": [
    "RANK_EMBEDDING_DIM = 64\n",
    "BATCH_SIZE = 2**12\n",
    "NEPOCH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImqKrQio47V1"
   },
   "outputs": [],
   "source": [
    "TRAIN_WEEK_NUM = 4\n",
    "WEEK_NUM = TRAIN_WEEK_NUM + 2\n",
    "\n",
    "VERSION_NAME = \"pivot\"\n",
    "TEST = False # * Set as `False` when do local experiments to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MS0MEj1z5Rxs"
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"/content/drive/MyDrive/HM-new/data/\")\n",
    "model_dir = Path(\"/content/drive/MyDrive/HM-new/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbasc4Yn7vjP"
   },
   "outputs": [],
   "source": [
    "dh = DataHelper(data_dir)\n",
    "data = dh.load_data(name=\"encoded_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dvqsn-x2NSVR"
   },
   "outputs": [],
   "source": [
    "inter = data['inter']\n",
    "inter = inter.loc[(inter.t_dat <= \"2020-08-19\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvLo7j94F0l6"
   },
   "source": [
    "## Calculate & Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-dWWWZ8F5Pk"
   },
   "outputs": [],
   "source": [
    "# article description - TFIDF - SVD\n",
    "if not os.path.exists(data_dir/'external/tfidf_item_embd.npy'):\n",
    "    articles = pd.read_csv(data_dir/'raw/articles.csv')\n",
    "\n",
    "    corpus = articles[[col for col in articles.columns if 'name' in col] + ['detail_desc']].T.apply(lambda x: ' '.join(map(str,x))).T\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=3)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    svd = TruncatedSVD(n_components=256, random_state=0)\n",
    "    tfidf_item = svd.fit_transform(X)\n",
    "    tfidf_item = np.concatenate([np.ones((1,256)), tfidf_item], axis=0)\n",
    "    tfidf_item.dump(data_dir/'external/tfidf_item_embd.npy')\n",
    "else:\n",
    "    tfidf_item = np.load(data_dir/'external/tfidf_item_embd.npy', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-36br0IjNpM7"
   },
   "outputs": [],
   "source": [
    "# article_id - customer_id TFIDF + SVD\n",
    "if not os.path.exists(data_dir/'external/tfidf_item_embd2.npy'):\n",
    "    corpus = inter.groupby('article_id').customer_id.apply(lambda x: ' '.join(map(str, x)))\n",
    "    article_ids = np.array(list(corpus.index))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=3)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    svd = TruncatedSVD(n_components=128, random_state=0)\n",
    "    X_svd = svd.fit_transform(X)\n",
    "\n",
    "    item_num = data['item']['article_id'].nunique()\n",
    "    tfidf_item2 = np.ones((item_num+1, 128)) / 128\n",
    "    for i,iid in enumerate(article_ids):\n",
    "        tfidf_item2[iid,:] = X_svd[i,:]\n",
    "\n",
    "    tfidf_item2.dump(data_dir/'external/tfidf_item_embd2.npy')\n",
    "else:\n",
    "    tfidf_item2 = np.load(data_dir/'external/tfidf_item_embd2.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QF8DZs-wMkUl"
   },
   "outputs": [],
   "source": [
    "# customer_id - product_code TFIDF + SVD\n",
    "if not os.path.exists(data_dir/'external/tfidf_user_embd.npy'):\n",
    "    inter = inter.merge(data['item'][['article_id','product_code']], on=['article_id'], how='left')\n",
    "    corpus = inter.groupby('customer_id').product_code.apply(lambda x: ' '.join(map(str, x)))\n",
    "    customer_ids = np.array(list(corpus.index))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=3)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    svd = TruncatedSVD(n_components=128, random_state=0)\n",
    "    X_svd = svd.fit_transform(X)\n",
    "\n",
    "    user_num = data['user']['customer_id'].nunique()\n",
    "    tfidf_user = np.ones((user_num+1, 128)) / 128\n",
    "    for i,uid in enumerate(customer_ids):\n",
    "        tfidf_user[uid,:] = X_svd[i,:]\n",
    "\n",
    "    tfidf_user.dump(data_dir/'external/tfidf_user_embd.npy')\n",
    "else:\n",
    "    tfidf_user = np.load(data_dir/'external/tfidf_user_embd.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M32FeEH_PS1w"
   },
   "outputs": [],
   "source": [
    "# * Load pre-trained embeddings\n",
    "w2v_user_embd = np.load(data_dir/'external'/'w2v_user_embd.npy', allow_pickle=True)\n",
    "w2v_item_embd = np.load(data_dir/'external'/'w2v_item_embd.npy', allow_pickle=True)\n",
    "w2v_product_embd = np.load(data_dir/'external'/'w2v_product_embd.npy', allow_pickle=True)\n",
    "image_item_embd = np.load(data_dir/'external'/'image_embd.npy', allow_pickle=True)\n",
    "w2v_sg_user_embd = np.load(data_dir/'external'/'w2v_skipgram_user_embd.npy', allow_pickle=True)\n",
    "w2v_sg_item_embd = np.load(data_dir/'external'/'w2v_skipgram_item_embd.npy', allow_pickle=True)\n",
    "w2v_sg_product_embd = np.load(data_dir/'external'/'w2v_skipgram_product_embd.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiszC35_47or"
   },
   "source": [
    "## Load Candidates & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2VSn73iGlA2"
   },
   "outputs": [],
   "source": [
    "# rule_feats = [\n",
    "#     'ItemPairRetrieve_1',\n",
    "#     'ItemPairRetrieve_2',\n",
    "#     'ItemPairRetrieve_3',\n",
    "#     'ItemPairRetrieve_4',\n",
    "#     'OrderHistoryDecay_1',\n",
    "#     'OrderHistoryDecay_2',\n",
    "#     'OrderHistory_1',\n",
    "#     'OrderHistory_2',\n",
    "#     'SaleTrend_1',\n",
    "#     'TimeHistoryDecay_1',\n",
    "#     'TimeHistoryDecay_2',\n",
    "#     'TimeHistory_1',\n",
    "#     'TimeHistory_2',\n",
    "#     'UGSaleTrend_1',\n",
    "#     'UGTimeHistory_1',\n",
    "#     'UGTimeHistory_2'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7KbLtnk4z0z",
    "outputId": "e42e3afb-631e-4edc-df54-f18741e64d40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "candidates = {}\n",
    "labels = {}\n",
    "for i in tqdm(range(1, WEEK_NUM)):\n",
    "    candidates[i] = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_candidate.pqt\")\n",
    "    labels[i] = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_label.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkNhkfuq5EPw"
   },
   "outputs": [],
   "source": [
    "feats = [\n",
    "    x\n",
    "    for x in candidates[1].columns\n",
    "    if x\n",
    "    not in [\n",
    "        \"label\",\n",
    "        \"sales_channel_id\",\n",
    "        \"t_dat\",\n",
    "        \"week\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "ids = [\"customer_id\", \"article_id\", \"product_code\"]\n",
    "dense_feats = [x for x in feats if x not in ids]\n",
    "# feats = ids + cat_features + dense_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIfny2Isklqs",
    "outputId": "f6c014fb-4317-4864-fdc1-9df8ccb2c86b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:30<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(dense_feats):\n",
    "    for i in range(1,WEEK_NUM):\n",
    "        if f in candidates[i].columns:\n",
    "            candidates[i][f] = candidates[i][f].astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYWW99nD556M"
   },
   "outputs": [],
   "source": [
    "full_data = pd.concat([candidates[i] for i in range(1,WEEK_NUM)], ignore_index=True)\n",
    "full_data = full_data[feats+['week','label']]\n",
    "gc.collect()\n",
    "# for f in tqdm(rule_feats):\n",
    "#     full_data[f] = full_data.groupby(['week','customer_id'])[f].rank()\n",
    "train = full_data[full_data['week']>1]\n",
    "valid = full_data[full_data['week']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVcq5-t7zreT",
    "outputId": "fa0fa69d-fbad-4913-ecf1-c14da162492d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del candidates\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRJNcFdr9-45"
   },
   "outputs": [],
   "source": [
    "# Standardize\n",
    "# for feat in dense_feats:\n",
    "    # mask = train[feat].notnull()\n",
    "    # value = train.loc[mask, feat].mean()\n",
    "    # train[feat] = train[feat].fillna(value)\n",
    "    # valid[feat] = valid[feat].fillna(value)\n",
    "    # scaler = MinMaxScaler().fit(train[feat].values.reshape(-1,1))\n",
    "    # train[feat] = scaler.transform(train[feat].values.reshape(-1,1))\n",
    "    # valid[feat] = scaler.transform(valid[feat].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8-Ohirc6SWH"
   },
   "outputs": [],
   "source": [
    "feat_dim = {}\n",
    "for feat in ids:\n",
    "    if feat in data['user'].columns:\n",
    "        feat_dim[feat] = int(data['user'][feat].max()) + 1\n",
    "    elif feat in data['item'].columns:\n",
    "        feat_dim[feat] = int(data['item'][feat].max()) + 1\n",
    "    else:\n",
    "        feat_dim[feat] = int(full_data[feat].max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9cISY640Y8x",
    "outputId": "a4675d31-3ec3-4a9e-ee32-d7891f7c5703"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del full_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqVs0eQC0FRY",
    "outputId": "c69b6014-82da-4613-cacb-4b6e3af3845e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [00:26,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train1 = train[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
    "X_train2 = np.zeros((X_train1.shape[0], len(dense_feats)), dtype='float32')\n",
    "for i,f in tqdm(enumerate(dense_feats)):\n",
    "    X_train2[:, i] = np.nan_to_num(train[f].values).astype('float32')\n",
    "    del train[f]\n",
    "y_train = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54XxZ1OP0Scb",
    "outputId": "95cd385a-8c70-4914-e05e-3b0cb181b14c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [00:08, 11.70it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test1 = valid[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
    "X_test2 = np.zeros((X_test1.shape[0], len(dense_feats)), dtype='float32')\n",
    "for i,f in tqdm(enumerate(dense_feats)):\n",
    "    X_test2[:, i] = np.nan_to_num(valid[f].values).astype('float32')\n",
    "    del valid[f]\n",
    "y_test = valid['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxUVZlxJ0Y8y"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1ZH721w5pSG"
   },
   "outputs": [],
   "source": [
    "customer_embd_layer_1 = Embedding(\n",
    "    feat_dim[\"customer_id\"], 64, weights=[w2v_sg_user_embd], trainable=False\n",
    ")\n",
    "customer_embd_layer_2 = Embedding(\n",
    "    feat_dim[\"customer_id\"], 64, weights=[w2v_user_embd], trainable=False\n",
    ")\n",
    "customer_embd_layer_3 = Embedding(\n",
    "    feat_dim[\"customer_id\"], 128, weights=[tfidf_user], trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1S2eMLNp5H5x"
   },
   "outputs": [],
   "source": [
    "article_embd_layer_1 = Embedding(\n",
    "    feat_dim[\"article_id\"], 64, weights=[w2v_sg_item_embd], trainable=False\n",
    ")\n",
    "\n",
    "article_embd_layer_2 = Embedding(\n",
    "    feat_dim[\"article_id\"], 64, weights=[w2v_item_embd], trainable=False\n",
    ")\n",
    "\n",
    "article_embd_layer_3 = Embedding(\n",
    "    feat_dim[\"article_id\"], 256, weights=[tfidf_item], trainable=False\n",
    ")\n",
    "\n",
    "article_embd_layer_4 = Embedding(\n",
    "    feat_dim[\"article_id\"], 128, weights=[tfidf_item2], trainable=False\n",
    ")\n",
    "\n",
    "article_embd_layer_5 = Embedding(\n",
    "    feat_dim[\"article_id\"], 512, weights=[image_item_embd], trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfru5_xrg8qh"
   },
   "outputs": [],
   "source": [
    "product_embd_layer_1 = Embedding(\n",
    "    feat_dim[\"product_code\"], 64, weights=[w2v_sg_product_embd], trainable=False\n",
    ")\n",
    "product_embd_layer_2 = Embedding(\n",
    "    feat_dim[\"product_code\"], 64, weights=[w2v_product_embd], trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_oDxhjt7rR6"
   },
   "outputs": [],
   "source": [
    "class FM(tf.keras.layers.Layer):\n",
    "    \"\"\"Factorization Machine\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.linear = None\n",
    "        self.w_0 = None\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.linear = Dense(1, use_bias=False)\n",
    "        self.w_0 = self.add_weight(\n",
    "            shape=(1,),\n",
    "            initializer=tf.keras.initializers.Zeros,\n",
    "            dtype=tf.float32,\n",
    "            trainable=True,\n",
    "            name=\"W_0\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, mask=None, *args, **kwargs):\n",
    "        # * inputs: (batch_size, num_of_fields, embedding_dim)\n",
    "        # * part2: (batch_size, 1)\n",
    "        part2 = tf.reduce_sum(self.linear(inputs), axis=1, keepdims=False)\n",
    "\n",
    "        # * square_sum: (batch_size, embedding_dim)\n",
    "        # * sum_square: (batch_size, embedding_dim)\n",
    "        square_sum = tf.square(tf.reduce_sum(inputs, axis=1, keepdims=False))\n",
    "        sum_square = tf.reduce_sum(inputs * inputs, axis=1, keepdims=False)\n",
    "        \n",
    "        # * part3: (batch_size, 1)\n",
    "        part3 = square_sum - sum_square\n",
    "        part3 = 0.5 * tf.reduce_sum(part3, axis=1, keepdims=True)\n",
    "        return tf.nn.bias_add(part2 + part3, self.w_0)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psg5LLL94URR",
    "outputId": "98d3578a-f818-48fd-ae50-85d557542880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)           (None, 3)            0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None,)             0           ['tf.cast[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None,)             0           ['tf.cast[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None,)             0           ['tf.cast[0][0]']                \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None,)             0           ['tf.cast[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None,)             0           ['tf.cast[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None,)             0           ['tf.cast[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 256)          27019008    ['tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None,)             0           ['tf.cast[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 512)          54038016    ['tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None,)             0           ['tf.cast[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None,)             0           ['tf.cast[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 64)           87806784    ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 64)           87806784    ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 64)           6754752     ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 64)           6754752     ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          32896       ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 128)          13509504    ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          65664       ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, 64)           3022400     ['tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 64)           3022400     ['tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 96)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 768)          0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]',            \n",
      "                                                                  'embedding_3[0][0]',            \n",
      "                                                                  'embedding_4[0][0]',            \n",
      "                                                                  'dense[0][0]',                  \n",
      "                                                                  'embedding_6[0][0]',            \n",
      "                                                                  'dense_1[0][0]',                \n",
      "                                                                  'embedding_8[0][0]',            \n",
      "                                                                  'embedding_9[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 96)          384         ['input_2[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 864)          0           ['concatenate[0][0]',            \n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 864)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          442880      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          131328      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 1120)         0           ['dense_3[0][0]',                \n",
      "                                                                  'concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 1120)         0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            1121        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 290,408,673\n",
      "Trainable params: 674,081\n",
      "Non-trainable params: 289,734,592\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs1 = Input(shape=X_train1.shape[1:], dtype=tf.int64)\n",
    "inputs2 = Input(shape=X_train2.shape[1:], dtype=tf.float32)\n",
    "input1 = tf.cast(inputs1, dtype=tf.int64)\n",
    "\n",
    "x_c_id1 = customer_embd_layer_1(input1[:,0])\n",
    "x_c_id2 = customer_embd_layer_2(input1[:,0])\n",
    "x_c_id3 = customer_embd_layer_3(input1[:,0])\n",
    "\n",
    "x_a_id1 = article_embd_layer_1(input1[:,1])\n",
    "x_a_id2 = article_embd_layer_2(input1[:,1])\n",
    "x_a_id3 = article_embd_layer_3(input1[:,1])\n",
    "x_a_id3 = Dense(128)(x_a_id3)\n",
    "x_a_id4 = article_embd_layer_4(input1[:,1])\n",
    "x_a_id5 = article_embd_layer_5(input1[:,1])\n",
    "x_a_id5 = Dense(128)(x_a_id5)\n",
    "\n",
    "x_p_id1 = product_embd_layer_1(input1[:,2])\n",
    "x_p_id2 = product_embd_layer_2(input1[:,2])\n",
    "\n",
    "\n",
    "x_id = Concatenate(axis=-1)([\n",
    "    x_c_id1, x_c_id2,\n",
    "    x_a_id1, x_a_id2, x_a_id3, x_a_id4, x_a_id5,\n",
    "    x_p_id1, x_p_id2,\n",
    "])\n",
    "\n",
    "x0 = Concatenate(axis=-1)([x_id, BatchNormalization()(inputs2)])\n",
    "# x = Dropout(0.2)(x0)\n",
    "# x = Dense(1024, activation='swish')(x)\n",
    "x = Dropout(0.2)(x0)\n",
    "x = Dense(512, activation='swish')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation='swish')(x)\n",
    "\n",
    "x = Concatenate(axis=-1)([x, x0])\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# x_c_id2_expand = tf.expand_dims(x_c_id2, axis=1)\n",
    "# x_a_id2_expand = tf.expand_dims(x_a_id2, axis=1)\n",
    "# x_p_id2_expand = tf.expand_dims(x_p_id2, axis=1)\n",
    "# fm_output = FM()(Concatenate(axis=1)([x_c_id2_expand, x_a_id2_expand, x_p_id2_expand]))\n",
    "# output = output + fm_output\n",
    "# output = Activation('sigmoid')(output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputs1, inputs2], outputs=[output])\n",
    "model.summary()\n",
    "    \n",
    "model.compile(\n",
    "    tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4),\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Um4beR62FC"
   },
   "outputs": [],
   "source": [
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=10, mode='max')\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=model_dir/'model_nn.h5',\n",
    "#     save_weights_only=True,\n",
    "#     monitor='val_auc',\n",
    "#     mode='max',\n",
    "#     save_best_only=True)\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train1, X_train2], y_train.astype(int), \n",
    "#     shuffle=True,\n",
    "#     batch_size=2048,\n",
    "#     validation_data=([X_test1, X_test2], y_test.astype(int)),\n",
    "#     epochs=30,\n",
    "#     callbacks=[checkpoint, early_stop]\n",
    "# )\n",
    "# # 0.7114\n",
    "# # 0.7294\n",
    "# # 0.7382\n",
    "# # 0.7565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGW114IL7I9C"
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_dir/'model_nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXOcPpTD9S0z"
   },
   "outputs": [],
   "source": [
    "probs = model.predict([X_test1, X_test2], batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3qRJElkI2Jk"
   },
   "outputs": [],
   "source": [
    "label = data['inter'][data['inter']['t_dat']>='2020-09-16']\n",
    "label = label.groupby('customer_id')['article_id'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POdGXQWpJicG"
   },
   "outputs": [],
   "source": [
    "valid['prob'] = probs\n",
    "pred = valid.sort_values(by='prob',ascending=False).reset_index(drop=True)\n",
    "pred = pred.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
    "pred.columns = ['customer_id','prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMl0xTnfc2D_"
   },
   "outputs": [],
   "source": [
    "valid = valid[['customer_id','article_id','prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFYhQSVPc8nJ"
   },
   "outputs": [],
   "source": [
    "valid.to_parquet(data_dir/'external'/'nn_valid.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWxE9Yp7JvRa"
   },
   "outputs": [],
   "source": [
    "label = label.merge(pred, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfQX9y5iJy6x",
    "outputId": "eab718fc-3385-4d8d-c2a1-f0697e62cb6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03129488004637625"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_at_k(label['article_id'], label['prediction'], k=12)\n",
    "# 0.028500554033301987\n",
    "# 0.029904528760153\n",
    "\n",
    "# 0.031648009478868075\n",
    "# 0.031309369857160076\n",
    "\n",
    "# 031769005497044554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bk7Z1JIsSkFW"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRkjESDvSlrr"
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_dir/'model_nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMSekWFSRO11"
   },
   "outputs": [],
   "source": [
    "class TQDMPredictCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, custom_tqdm_instance=None, tqdm_cls=tqdm, **tqdm_params):\n",
    "        super().__init__()\n",
    "        self.tqdm_cls = tqdm_cls\n",
    "        self.tqdm_progress = None\n",
    "        self.prev_predict_batch = None\n",
    "        self.custom_tqdm_instance = custom_tqdm_instance\n",
    "        self.tqdm_params = tqdm_params\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        self.tqdm_progress.update(batch - self.prev_predict_batch)\n",
    "        self.prev_predict_batch = batch\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        self.prev_predict_batch = 0\n",
    "        if self.custom_tqdm_instance:\n",
    "            self.tqdm_progress = self.custom_tqdm_instance\n",
    "            return\n",
    "\n",
    "        total = self.params.get('steps')\n",
    "        if total:\n",
    "            total -= 1\n",
    "\n",
    "        self.tqdm_progress = self.tqdm_cls(total=total, **self.tqdm_params)\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        if self.tqdm_progress and not self.custom_tqdm_instance:\n",
    "            self.tqdm_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpTOgjxSSqRJ",
    "outputId": "03b8c3be-b153-4be8-8c3f-45a24c281f7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, valid, X_train1, X_train2, X_test1, X_test2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYGqgcXb0ZWa"
   },
   "outputs": [],
   "source": [
    "chunk = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Dd8Mk_UdfcV",
    "outputId": "4b95775a-d083-4e4b-e68f-743f45f9ec5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [01:12<00:00,  1.33it/s]\n",
      "96it [01:26,  1.12it/s]\n",
      "100%|██████████| 13503/13503 [03:07<00:00, 71.85it/s]\n"
     ]
    }
   ],
   "source": [
    "test_candidates = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week0_candidate_{chunk}.pqt\")\n",
    "for f in tqdm(dense_feats):\n",
    "    test_candidates[f] = test_candidates[f].astype('float16')\n",
    "test1 = test_candidates[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
    "test2 = np.zeros((test1.shape[0], len(dense_feats)), dtype='float32')\n",
    "for i,f in tqdm(enumerate(dense_feats)):\n",
    "    test2[:, i] = np.nan_to_num(test_candidates[f].values).astype('float32')\n",
    "    del test_candidates[f]\n",
    "gc.collect()\n",
    "\n",
    "probs = model.predict([test1, test2], batch_size=2048, callbacks=[TQDMPredictCallback()])\n",
    "# test_candidates = pd.concat([test_candidates, test_candidates2], ignore_index=True)\n",
    "test_candidates[\"prob\"] = probs\n",
    "pred_lgb = test_candidates[['customer_id','article_id','prob']]\n",
    "# pred_lgb = pred_lgb.sort_values(by=[\"customer_id\",\"prob\"], ascending=False).reset_index(drop=True)\n",
    "pred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\n",
    "# pred_lgb = pred_lgb.drop_duplicates(['customer_id', 'prediction'], keep='first')\n",
    "pred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofTfBe310hD3"
   },
   "outputs": [],
   "source": [
    "pred_lgb.to_parquet(data_dir/'interim'/f'nn_test_{chunk}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83ftWTZN5z95"
   },
   "outputs": [],
   "source": [
    "test_pred1 = pd.read_parquet(data_dir/'interim'/f'nn_test_0.pqt')\n",
    "test_pred2 = pd.read_parquet(data_dir/'interim'/f'nn_test_1.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgEvjLJvYFry"
   },
   "outputs": [],
   "source": [
    "test_pred = pd.concat([test_pred1, test_pred2], ignore_index=True)\n",
    "test_pred = test_pred.sort_values(by=[\"prob\"], ascending=False).reset_index(drop=True)\n",
    "test_pred = test_pred.drop_duplicates(['customer_id', 'prediction'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjx-3arOaAx6"
   },
   "outputs": [],
   "source": [
    "test_pred.to_parquet(data_dir/'processed'/'nn_test.pqt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
